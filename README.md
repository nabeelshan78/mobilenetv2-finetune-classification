# MobileNetV2 Fine-Tuning for Binary Classification (Alpaca vs Not-Alpaca)

This project demonstrates **transfer learning and fine-tuning using MobileNetV2** on a small binary classification task - distinguishing between Alpaca and Not-Alpaca images.

## ðŸ“Š MobileNetV2 Architecture

Below is a visual representation of the MobileNetV2 architecture used in this project:

![MobileNetV2 Architecture](images/mobilenetv2.png)


The goal was not just to train a model, but to deeply understand the full transfer learning pipeline, including:

- Pre-trained model loading
- Feature extraction
- Fine-tuning select layers
- Data augmentation
- Performance tracking

---

## ðŸ“ Directory Structure

```bash
transfer_learning/
â”œâ”€â”€ notebook.ipynb # Full training + fine-tuning workflow
â”œâ”€â”€ dataset/
â”‚ â”œâ”€â”€ alpaca/ # Alpaca images
â”‚ â””â”€â”€ not alpaca/ # Not-Alpaca images
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ accuracy_loss_train_val.png # Before fine-tuning
â”‚ â”œâ”€â”€ ft_acc_loss_train_val.png # After fine-tuning
â”‚ â”œâ”€â”€ val_metrics.png # Final bar plot of validation metrics
â”‚ â””â”€â”€ mobilenetv2.png # Architecture illustration

```


---

## ðŸš€ Whatâ€™s Inside

### âœ… Model:
- Base: `MobileNetV2` (ImageNet pretrained, top removed)
- Custom head: `GlobalAveragePooling2D` + `Dropout(0.2)` + `Dense(1, activation='sigmoid')`

### ðŸ§ª Training Pipeline:
- **Data Augmentation** (flips, rotations, rescale)
- **Feature Extraction** (base frozen)
- **Fine-Tuning** (top layers `after 120 layers` of MobileNetV2 unfrozen)
- Optimized with `Adam`, binary crossentropy loss

---

## ðŸ“Š Training & Fine-Tuning Results

### â–¶ï¸ Before Fine-Tuning:
![Training Acc/Loss](images/accuracy_loss_train_val.png)

### ðŸ” After Fine-Tuning:
![Fine-Tuned Acc/Loss](images/ft_acc_loss_train_val.png)

### ðŸ“ˆ Final Validation Metrics:
![Validation Metrics](images/val_metrics.png)

---

## ðŸ§  Why This Project Matters

This project was about *doing things the right way*:

- No â€œblack-boxâ€ shortcut - every layer was handled intentionally
- Understood how freezing/unfreezing impacts convergence
- Evaluated the effect of fine-tuning using actual metrics and visualizations

This kind of depth is what I aim for in every ML project.

---

## ðŸ› ï¸ How to Run

1. Clone the repo
2. Put your images inside `dataset/alpaca` and `dataset/not alpaca`
3. Open and run `notebook.ipynb` step-by-step
4. (Optional) Visualize and extend the architecture or export the model

---

## ðŸ“Œ Tools Used

- TensorFlow / Keras
- Matplotlib
- MobileNetV2 weights (without top)
- Jupyter Notebook

---

## ðŸ¤ Let's Connect

If you're working on transfer learning or deep learning model optimization, Iâ€™d love to connect and collaborate. Feel free to fork this repo or open an issue to discuss improvements or ideas!

---

## ðŸ‘¨â€ðŸ’» Author

**Nabeel Shan**  
Software Engineering Student @ NUST Islamabad  
Aspiring AI Researcher . AI/ML Enthusiast . Deep Learning Explorer  
ðŸ”— [LinkedIn](https://www.linkedin.com/in/nabeelshan) â€¢ [GitHub](https://github.com/nabeelshan78)
> Currently mastering Deep Learning architectures through hands-on work
> Open to research collaborations, open-source contributions, and AI/ML projects.
